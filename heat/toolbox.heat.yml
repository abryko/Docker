#
##
### Written by the CAT team
##
#
heat_template_version: 2013-05-23

description: High Aviability Toolbox stack for Cloudwatt


parameters:
  keypair_name:
    description: Keypair to inject in instance
    label: SSH Keypair
    type: string

  os_username:
    description: OpenStack Username
    label: OpenStack Username
    type: string

  os_password:
    description: OpenStack Password
    label: OpenStack Password
    type: string

  os_tenant:
    description: OpenStack Tenant Name
    label: OpenStack Tenant Name
    type: string

  os_auth:
    description: OpenStack Auth URL
    default: https://identity.fr1.cloudwatt.com/v2.0
    label: OpenStack Auth URL
    type: string

  domain:
    description: Wildcarded domain, ex example.com must have a *.example.com DNS entry
    label: Cloud DNS
    type: string

  flavor_name:
    default: n1.cw.standard-4
    description: Flavor to use for the deployed instance
    type: string
    label: Instance Type (Flavor)
    constraints:
      - allowed_values:
          - s1.cw.small-1
          - n1.cw.standard-1
          - n1.cw.standard-2
          - n1.cw.standard-4
          - n1.cw.standard-8
          - n1.cw.standard-12
          - n1.cw.standard-16

resources:
  network:
    type: OS::Neutron::Net

  subnet:
    type: OS::Neutron::Subnet
    properties:
      network_id: { get_resource: network }
      ip_version: 4
      dns_nameservers:
        - 10.0.1.254
        - 185.23.94.244
        - 185.23.94.245
      cidr: 10.0.1.0/24
      allocation_pools:
        - { start: 10.0.1.100, end: 10.0.1.199 }

  router:
    type: OS::Neutron::Router
    properties:
      admin_state_up: true
      external_gateway_info:
        enable_snat: true
        network: "public"

  toolbox_interface:
    type: OS::Neutron::RouterInterface
    properties:
      router_id: { get_resource: router }
      subnet_id: { get_resource: subnet }

  security_group:
    type: OS::Neutron::SecurityGroup
    properties:
      rules:
        - { direction: ingress }
        - { direction: egress }

  master_port:
    type: OS::Neutron::Port
    properties:
      network: { get_resource: network }
      fixed_ips:
        - ip_address: 10.0.1.254
          subnet_id: { get_resource: subnet }
      security_groups:
        - { get_resource: security_group }

  master:
    type: OS::Nova::Server
    properties:
      key_name: { get_param: keypair_name }
      image: 811c20f7-ad34-4401-85a9-72b9f921a0dd
      flavor: { get_param: flavor_name }
      user_data_format: RAW
      networks:
        - port: { get_resource: master_port }
      user_data:
        str_replace:
          params:
            $private_ipv4: { get_attr: [ master_port, fixed_ips, 0, ip_address ] }
            $public_ipv4: { get_attr: [floating_ip, floating_ip_address] }
            $domain: { get_param: domain }
            $os_username: { get_param: os_username }
            $os_password: { get_param: os_password }
            $os_tenant: { get_param: os_tenant }
            $os_auth: { get_param: os_auth }
          template: |
            #cloud-config
            write_files:
              - path: /etc/flannel/options.env
                permissions: 0644
                owner: "root:root"
                content: |
                  FLANNELD_IFACE=$private_ipv4
                  FLANNELD_ETCD_ENDPOINTS=http://$private_ipv4:2379
              - path: /opt/bin/kubelet-wrapper
                permissions: 0700
                owner: "root:root"
                content: |
                  #!/bin/bash
                  # Wrapper for launching kubelet via rkt-fly stage1.
                  #
                  # Make sure to set KUBELET_VERSION to an image tag published here:
                  # https://quay.io/repository/coreos/hyperkube?tab=tags Alternatively,
                  # override $KUBELET_ACI to a custom location.

                  set -e

                  if [ -z "${KUBELET_VERSION}" ]; then
                      echo "ERROR: must set KUBELET_VERSION"
                      exit 1
                  fi

                  KUBELET_ACI="${KUBELET_ACI:-quay.io/coreos/hyperkube}"

                  mkdir --parents /etc/kubernetes
                  mkdir --parents /var/lib/docker
                  mkdir --parents /var/lib/kubelet
                  mkdir --parents /run/kubelet

                  exec /usr/bin/rkt run \
                    --volume etc-kubernetes,kind=host,source=/etc/kubernetes \
                    --volume etc-ssl-certs,kind=host,source=/usr/share/ca-certificates \
                    --volume var-lib-docker,kind=host,source=/var/lib/docker \
                    --volume var-lib-kubelet,kind=host,source=/var/lib/kubelet \
                    --volume run,kind=host,source=/run \
                    --mount volume=etc-kubernetes,target=/etc/kubernetes \
                    --mount volume=etc-ssl-certs,target=/etc/ssl/certs \
                    --mount volume=var-lib-docker,target=/var/lib/docker \
                    --mount volume=var-lib-kubelet,target=/var/lib/kubelet \
                    --mount volume=run,target=/run \
                    --trust-keys-from-https \
                    $RKT_OPTS \
                    --stage1-path=/usr/share/rkt/stage1-fly.aci \
                    ${KUBELET_ACI}:${KUBELET_VERSION} --exec=/kubelet -- "$@"
              - path: /opt/flannel-init.sh
                permissions: 0700
                owner: "root:root"
                content: |
                  #!/bin/bash
                  echo "Waiting for etcd..."
                  ETCD="http://$private_ipv4:2379"
                  while true
                  do
                      echo "Trying: $ETCD"
                      if [ -n "$(curl --silent "$ETCD/v2/machines")" ]; then
                          ACTIVE_ETCD=$ETCD
                          break
                      fi
                      sleep 1
                      if [ -n "$ACTIVE_ETCD" ]; then
                          break
                      fi
                  done
                  RES=$(curl --silent -X PUT -d "value={\"Network\":\"10.1.0.0/16\",\"Backend\":{\"Type\":\"vxlan\"}}" "$ACTIVE_ETCD/v2/keys/coreos.com/network/config?prevExist=false")
                  if [ -z "$(echo $RES | grep '"action":"create"')" ] && [ -z "$(echo $RES | grep 'Key already exists')" ]; then
                      echo "Unexpected error configuring flannel pod network: $RES"
                  fi
              - path: /opt/kube-namespace-init.sh
                permissions: 0700
                owner: "root:root"
                content: |
                  #!/bin/bash
                  echo "Waiting for Kubernetes API..."
                  K8S="http://$private_ipv4:8080"
                  while true
                  do
                      echo "Trying: $K8S"
                      if [ -n "$(curl --silent "$K8S/version")" ]; then
                          ACTIVE_K8S=$K8S
                          break
                      fi
                      sleep 1
                      if [ -n "$ACTIVE_K8S" ]; then
                          break
                      fi
                  done
                  RES=$(curl -H "Content-Type: application/json" -XPOST -d'{"apiVersion":"v1","kind":"Namespace","metadata":{"name":"kube-system"}}' "http://127.0.0.1:8080/api/v1/namespaces")
                  if [ -z "$(echo $RES | grep '"phase": "Active"')" ]; then
                      echo "Unexpected error configuring Kubernetes Namespace : $RES"
                  else
                      echo "Created kube-system namespace"
                      mkdir -p /opt/bin
                      curl -o /opt/bin/kubectl https://storage.googleapis.com/kubernetes-release/release/v1.2.0/bin/linux/amd64/kubectl
                      chmod +x /opt/bin/kubectl
                      /opt/bin/kubectl create -f /etc/kubernetes/descriptors
                  fi
              - path: /etc/kubernetes/manifests/kube-apiserver.yaml
                permissions: 0666
                owner: "root:root"
                content: |
                  apiVersion: v1
                  kind: Pod
                  metadata:
                    name: kube-apiserver
                    namespace: kube-system
                  spec:
                    hostNetwork: true
                    containers:
                    - name: kube-apiserver
                      image: quay.io/coreos/hyperkube:v1.2.0_coreos.0
                      command:
                      - /hyperkube
                      - apiserver
                      - --bind-address=0.0.0.0
                      - --insecure-bind-address=0.0.0.0
                      - --etcd-servers=http://$private_ipv4:2379
                      - --cloud-provider=openstack
                      - --cloud-config=/openstack.conf
                      - --allow-privileged=true
                      - --service-cluster-ip-range=10.0.2.0/24
                      - --secure-port=443
                      - --advertise-address=$private_ipv4
                      - --admission-control=NamespaceLifecycle,NamespaceExists,LimitRanger,SecurityContextDeny,ServiceAccount,ResourceQuota
                      - --tls-cert-file=/etc/kubernetes/ssl/apiserver.pem
                      - --tls-private-key-file=/etc/kubernetes/ssl/apiserver-key.pem
                      - --client-ca-file=/etc/kubernetes/ssl/ca.pem
                      - --service-account-key-file=/etc/kubernetes/ssl/apiserver-key.pem
                      ports:
                      - containerPort: 443
                        hostPort: 443
                        name: https
                      - containerPort: 8080
                        hostPort: 8080
                        name: local
                      volumeMounts:
                      - mountPath: /etc/kubernetes/ssl
                        name: ssl-certs-kubernetes
                        readOnly: true
                      - mountPath: /openstack.conf
                        name: openstack-config
                        readOnly: true
                      - mountPath: /etc/ssl/certs
                        name: ssl-certs-host
                        readOnly: true
                    volumes:
                    - hostPath:
                        path: /etc/kubernetes/openstack.conf
                      name: openstack-config
                    - hostPath:
                        path: /etc/kubernetes/ssl
                      name: ssl-certs-kubernetes
                    - hostPath:
                        path: /usr/share/ca-certificates
                      name: ssl-certs-host
              - path: /etc/kubernetes/manifests/kube-proxy.yaml
                permissions: 0666
                owner: "root:root"
                content: |
                  apiVersion: v1
                  kind: Pod
                  metadata:
                    name: kube-proxy
                    namespace: kube-system
                  spec:
                    hostNetwork: true
                    containers:
                    - name: kube-proxy
                      image: quay.io/coreos/hyperkube:v1.2.0_coreos.0
                      command:
                      - /hyperkube
                      - proxy
                      - --master=http://127.0.0.1:8080
                      - --proxy-mode=iptables
                      securityContext:
                        privileged: true
                      volumeMounts:
                      - mountPath: /etc/ssl/certs
                        name: ssl-certs-host
                        readOnly: true
                    volumes:
                    - hostPath:
                        path: /usr/share/ca-certificates
                      name: ssl-certs-host
              - path: /etc/kubernetes/manifests/kube-podmaster.yaml
                permissions: 0666
                owner: "root:root"
                content: |
                  apiVersion: v1
                  kind: Pod
                  metadata:
                    name: kube-podmaster
                    namespace: kube-system
                  spec:
                    hostNetwork: true
                    containers:
                    - name: scheduler-elector
                      image: gcr.io/google_containers/podmaster:1.1
                      command:
                      - /podmaster
                      - --etcd-servers=http://$private_ipv4:2379
                      - --key=scheduler
                      - --whoami=${ADVERTISE_IP}
                      - --source-file=/src/manifests/kube-scheduler.yaml
                      - --dest-file=/dst/manifests/kube-scheduler.yaml
                      volumeMounts:
                      - mountPath: /src/manifests
                        name: manifest-src
                        readOnly: true
                      - mountPath: /dst/manifests
                        name: manifest-dst
                    - name: controller-manager-elector
                      image: gcr.io/google_containers/podmaster:1.1
                      command:
                      - /podmaster
                      - --etcd-servers=http://$private_ipv4:2379
                      - --key=controller
                      - --whoami=$private_ipv4
                      - --source-file=/src/manifests/kube-controller-manager.yaml
                      - --dest-file=/dst/manifests/kube-controller-manager.yaml
                      terminationMessagePath: /dev/termination-log
                      volumeMounts:
                      - mountPath: /src/manifests
                        name: manifest-src
                        readOnly: true
                      - mountPath: /dst/manifests
                        name: manifest-dst
                    volumes:
                    - hostPath:
                        path: /srv/kubernetes/manifests
                      name: manifest-src
                    - hostPath:
                        path: /etc/kubernetes/manifests
                      name: manifest-dst
              - path: /etc/kubernetes/manifests/kube-controller-manager.yaml
                permissions: 0666
                owner: "root:root"
                content: |
                  apiVersion: v1
                  kind: Pod
                  metadata:
                    name: kube-controller-manager
                    namespace: kube-system
                  spec:
                    hostNetwork: true
                    containers:
                    - name: kube-controller-manager
                      image: quay.io/coreos/hyperkube:v1.2.0_coreos.0
                      command:
                      - /hyperkube
                      - controller-manager
                      - --cloud-provider=openstack
                      - --cloud-config=/openstack.conf
                      - --master=http://127.0.0.1:8080
                      - --service-account-private-key-file=/etc/kubernetes/ssl/apiserver-key.pem
                      - --root-ca-file=/etc/kubernetes/ssl/ca.pem
                      livenessProbe:
                        httpGet:
                          host: 127.0.0.1
                          path: /healthz
                          port: 10252
                        initialDelaySeconds: 15
                        timeoutSeconds: 1
                      volumeMounts:
                      - mountPath: /etc/kubernetes/ssl
                        name: ssl-certs-kubernetes
                        readOnly: true
                      - mountPath: /openstack.conf
                        name: openstack-config
                        readOnly: true
                      - mountPath: /etc/ssl/certs
                        name: ssl-certs-host
                        readOnly: true
                    volumes:
                    - hostPath:
                        path: /etc/kubernetes/openstack.conf
                      name: openstack-config
                    - hostPath:
                        path: /etc/kubernetes/ssl
                      name: ssl-certs-kubernetes
                    - hostPath:
                        path: /usr/share/ca-certificates
                      name: ssl-certs-host
              - path: /etc/kubernetes/manifests/kube-scheduler.yaml
                permissions: 0666
                owner: "root:root"
                content: |
                  apiVersion: v1
                  kind: Pod
                  metadata:
                    name: kube-scheduler
                    namespace: kube-system
                  spec:
                    hostNetwork: true
                    containers:
                    - name: kube-scheduler
                      image: quay.io/coreos/hyperkube:v1.2.0_coreos.0
                      command:
                      - /hyperkube
                      - scheduler
                      - --master=http://127.0.0.1:8080
                      livenessProbe:
                        httpGet:
                          host: 127.0.0.1
                          path: /healthz
                          port: 10251
                        initialDelaySeconds: 15
                        timeoutSeconds: 1
              - path: /etc/kubernetes/descriptors/skydns-rc.yaml
                permissions: 0666
                owner: "root:root"
                content: |
                  apiVersion: v1
                  kind: ReplicationController
                  metadata:
                    name: kube-dns-v11
                    namespace: kube-system
                    labels:
                      k8s-app: kube-dns
                      version: v11
                      kubernetes.io/cluster-service: "true"
                  spec:
                    replicas: 1
                    selector:
                      k8s-app: kube-dns
                      version: v11
                    template:
                      metadata:
                        labels:
                          k8s-app: kube-dns
                          version: v11
                          kubernetes.io/cluster-service: "true"
                      spec:
                        containers:
                        - name: etcd
                          image: gcr.io/google_containers/etcd-amd64:2.2.1
                          resources:
                            limits:
                              cpu: 100m
                              memory: 500Mi
                            requests:
                              cpu: 100m
                              memory: 50Mi
                          command:
                          - /usr/local/bin/etcd
                          - -data-dir
                          - /var/etcd/data
                          - -listen-client-urls
                          - http://127.0.0.1:2379,http://127.0.0.1:4001
                          - -advertise-client-urls
                          - http://127.0.0.1:2379,http://127.0.0.1:4001
                          - -initial-cluster-token
                          - skydns-etcd
                          volumeMounts:
                          - name: etcd-storage
                            mountPath: /var/etcd/data
                        - name: kube2sky
                          image: gcr.io/google_containers/kube2sky:1.14
                          resources:
                            limits:
                              cpu: 100m
                              memory: 200Mi
                            requests:
                              cpu: 100m
                              memory: 50Mi
                          livenessProbe:
                            httpGet:
                              path: /healthz
                              port: 8080
                              scheme: HTTP
                            initialDelaySeconds: 60
                            timeoutSeconds: 5
                            successThreshold: 1
                            failureThreshold: 5
                          readinessProbe:
                            httpGet:
                              path: /readiness
                              port: 8081
                              scheme: HTTP
                            initialDelaySeconds: 30
                            timeoutSeconds: 5
                          args:
                          # command = "/kube2sky"
                          - --domain=$domain
                        - name: skydns
                          image: gcr.io/google_containers/skydns:2015-10-13-8c72f8c
                          resources:
                            limits:
                              cpu: 100m
                              memory: 200Mi
                            requests:
                              cpu: 100m
                              memory: 50Mi
                          args:
                          - -machines=http://127.0.0.1:4001
                          - -addr=0.0.0.0:53
                          - -ns-rotate=false
                          - -domain=$domain.
                          ports:
                          - containerPort: 53
                            name: dns
                            protocol: UDP
                          - containerPort: 53
                            name: dns-tcp
                            protocol: TCP
                        - name: healthz
                          image: gcr.io/google_containers/exechealthz:1.0
                          resources:
                            # keep request = limit to keep this container in guaranteed class
                            limits:
                              cpu: 10m
                              memory: 20Mi
                            requests:
                              cpu: 10m
                              memory: 20Mi
                          args:
                          - -cmd=nslookup kubernetes.default.svc.$domain 127.0.0.1 >/dev/null
                          - -port=8080
                          ports:
                          - containerPort: 8080
                            protocol: TCP
                        volumes:
                        - name: etcd-storage
                          emptyDir: {}
                        dnsPolicy: Default  # Don't use cluster DNS.
              - path: /etc/kubernetes/descriptors/skydns-service.yaml
                permissions: 0666
                owner: "root:root"
                content: |
                  apiVersion: v1
                  kind: Service
                  metadata:
                    name: kube-dns
                    namespace: kube-system
                    labels:
                      k8s-app: kube-dns
                      kubernetes.io/cluster-service: "true"
                      kubernetes.io/name: "KubeDNS"
                  spec:
                    selector:
                      k8s-app: kube-dns
                    clusterIP: 10.0.2.2
                    ports:
                    - name: dns
                      port: 53
                      protocol: UDP
                    - name: dns-tcp
                      port: 53
                      protocol: TCP
              - path: /etc/kubernetes/descriptors/rabbitmq-service.yaml
                permissions: 0666
                owner: "root:root"
                content: |
                  apiVersion: v1
                  kind: Service
                  metadata:
                    labels:
                      component: rabbitmq
                    name: rabbitmq
                  spec:
                    ports:
                      - port: 5672
                    selector:
                      component: rabbitmq
              - path: /etc/kubernetes/descriptors/rabbitmq-rc.yaml
                permissions: 0666
                owner: "root:root"
                content: |
                  apiVersion: v1
                  kind: ReplicationController
                  metadata:
                    labels:
                      component: rabbitmq
                    name: rabbitmq-controller
                  spec:
                    replicas: 1
                    template:
                      metadata:
                        labels:
                          component: rabbitmq
                      spec:
                        containers:
                          - image: rabbitmq
                            name: rabbitmq
                            ports:
                              - containerPort: 5672
                                hostPort: 5672
                            resources:
                              limits:
                                cpu: 100m
              - path: /etc/kubernetes/descriptors/toolbox-service.yaml
                permissions: 0666
                owner: "root:root"
                content: |
                  apiVersion: v1
                  kind: Service
                  metadata:
                    labels:
                      component: toolbox
                    name: manager
                  spec:
                    ports:
                      - port: 80
                        targetPort: 3000
                        nodePort: 30000
                    type: NodePort
                    selector:
                      component: toolbox
              - path: /etc/kubernetes/descriptors/toolbox-rc.yaml
                permissions: 0666
                owner: "root:root"
                content: |
                  apiVersion: extensions/v1beta1
                  kind: Deployment
                  metadata:
                    labels:
                      component: toolbox
                    name: toolbox
                  spec:
                    replicas: 1
                    template:
                      metadata:
                        labels:
                          component: toolbox
                      spec:
                        containers:
                          - image: cloudwattfr/toolbox:latest
                            command:
                              - npm
                              - start
                              - $private_ipv4
                              - $public_ipv4
                            name: toolbox
                            env:
                              - name: DOMAIN
                                value: $domain
                            ports:
                              - containerPort: 3000
              - path: /etc/kubernetes/descriptors/rethinkdb-service.yaml
                permissions: 0666
                owner: "root:root"
                content: |
                  apiVersion: v1
                  kind: Service
                  metadata:
                    labels:
                      db: rethinkdb
                    name: rethinkdb
                  spec:
                    ports:
                      - port: 28015
                        targetPort: 28015
                    selector:
                      db: rethinkdb
              - path: /etc/kubernetes/descriptors/rethinkdb-rc.yaml
                permissions: 0666
                owner: "root:root"
                content: |
                  apiVersion: v1
                  kind: ReplicationController
                  metadata:
                    labels:
                      db: rethinkdb
                    name: rethinkdb-rc
                  spec:
                    replicas: 1
                    selector:
                      db: rethinkdb
                      role: replicas
                    template:
                      metadata:
                        labels:
                          db: rethinkdb
                          role: replicas
                      spec:
                        containers:
                          - image: cedbossneo/rethinkdb-kubernetes:latest
                            name: rethinkdb
                            env:
                              - name: POD_NAMESPACE
                                valueFrom:
                                  fieldRef:
                                    fieldPath: metadata.namespace
                            ports:
                              - containerPort: 8080
                                name: admin-port
                                hostPort: 8081
                              - containerPort: 28015
                                name: driver-port
                                hostPort: 28015
                              - containerPort: 29015
                                name: cluster-port
                            volumeMounts:
                              - mountPath: /data/rethinkdb_data
                                name: rethinkdb-storage
                        volumes:
                          - name: rethinkdb-storage
                            emptyDir: {}
              - path: /etc/environment
                permissions: 0644
                owner: "root:root"
                content: |
                  COREOS_PRIVATE_IPV4=$private_ipv4
                  COREOS_PUBLIC_IPV4=$public_ipv4
                  ETCD_ADDR=$private_ipv4:2379
                  ETCD_PEER_ADDR=$private_ipv4:2380
                  TOOLBOX_DOMAIN=$domain
              - path: /etc/kubernetes/openstack.conf
                permissions: 0644
                owner: "root:root"
                content: |
                  [Global]
                  auth-url=$os_auth
                  username=$os_username
                  password=$os_password
                  tenant-name=$os_tenant
              - path: /opt/kubernetes-init-ssl.sh
                permissions: 0700
                owner: "root:root"
                content: |
                  #!/bin/bash
                  mkdir -p /etc/kubernetes/ssl
                  cd /etc/kubernetes/ssl
                  # Config
                  cat <<EOF > openssl.cnf
                  [req]
                  req_extensions = v3_req
                  distinguished_name = req_distinguished_name
                  [req_distinguished_name]
                  [ v3_req ]
                  basicConstraints = CA:FALSE
                  keyUsage = nonRepudiation, digitalSignature, keyEncipherment
                  subjectAltName = @alt_names
                  [alt_names]
                  DNS.1 = kubernetes
                  DNS.2 = kubernetes.default
                  DNS.3 = kubernetes.default.svc
                  DNS.4 = kubernetes.default.svc.$domain
                  IP.1 = 10.0.2.1
                  IP.2 = $private_ipv4
                  EOF
                  # Root CA
                  openssl genrsa -out ca-key.pem 2048
                  openssl req -x509 -new -nodes -key ca-key.pem -days 10000 -out ca.pem -subj "/CN=kube-ca"
                  # Server CA
                  openssl genrsa -out apiserver-key.pem 2048
                  openssl req -new -key apiserver-key.pem -out apiserver.csr -subj "/CN=kube-apiserver" -config openssl.cnf
                  openssl x509 -req -in apiserver.csr -CA ca.pem -CAkey ca-key.pem -CAcreateserial -out apiserver.pem -days 365 -extensions v3_req -extfile openssl.cnf
                  # Admin CA
                  openssl genrsa -out admin-key.pem 2048
                  openssl req -new -key admin-key.pem -out admin.csr -subj "/CN=kube-admin"
                  openssl x509 -req -in admin.csr -CA ca.pem -CAkey ca-key.pem -CAcreateserial -out admin.pem -days 365
                  # Export CA to Etcd
                  curl --silent -X PUT --data-urlencode value@ca.pem "http://$private_ipv4:2379/v2/keys/ssl/ca"
                  curl --silent -X PUT --data-urlencode value@ca-key.pem "http://$private_ipv4:2379/v2/keys/ssl/key"
                  # Set permissions
                  chmod 600 /etc/kubernetes/ssl/*-key.pem
                  chown root:root /etc/kubernetes/ssl/*-key.pem

            coreos:
              etcd2:
                name: "%H"
                advertise-client-urls: http://$private_ipv4:2379
                initial-advertise-peer-urls: http://$private_ipv4:2380
                initial-cluster: "%H=http://$private_ipv4:2380"
                listen-client-urls: http://0.0.0.0:2379,http://0.0.0.0:4001
                listen-peer-urls: http://$private_ipv4:2380
              units:
                - name: etcd2.service
                  command: start
                - name: generatekey.service
                  command: start
                  content: |
                    [Unit]
                    ConditionPathExists=!/home/core/keys/key
                    Description=Toolbox KeyPair Generator

                    [Service]
                    Type=oneshot
                    ExecStartPre=-/usr/bin/mkdir /home/core/keys
                    ExecStart=/usr/bin/ssh-keygen -t rsa -f /home/core/keys/key -N ""
                - name: generatessl.service
                  command: start
                  content: |
                    [Unit]
                    Requires=etcd2.service
                    After=etcd2.service
                    ConditionPathExists=!/etc/kubernetes/ssl
                    Description=Kubernetes Keys Generator

                    [Service]
                    Type=oneshot
                    ExecStart=/opt/kubernetes-init-ssl.sh
                - name: flanneld.service
                  drop-ins:
                    - name: 40-ExecStartPre-symlink.conf
                      content: |
                        [Service]
                        ExecStartPre=/opt/flannel-init.sh
                        ExecStartPre=/usr/bin/ln -sf /etc/flannel/options.env /run/flannel/options.env
                - name: docker.service
                  drop-ins:
                    - name: 40-flannel.conf
                      content: |
                        [Unit]
                        Requires=flanneld.service
                        After=flanneld.service
                - name: kubelet.service
                  command: start
                  content: |
                    [Service]
                    ExecStartPre=/usr/bin/mkdir -p /etc/kubernetes/manifests
                    Environment=KUBELET_VERSION=v1.2.0_coreos.0
                    ExecStart=/opt/bin/kubelet-wrapper \
                      --api-servers=http://127.0.0.1:8080 \
                      --register-node=true \
                      --allow-privileged=true \
                      --config=/etc/kubernetes/manifests \
                      --hostname-override=$private_ipv4 \
                      --cluster-dns=10.0.2.2 \
                      --cluster-domain=$domain
                    ExecStartPost=-/opt/kube-namespace-init.sh
                    Restart=always
                    RestartSec=10
                    [Install]
                    WantedBy=multi-user.target
                - name: openvpn-init.service
                  command: start
                  content: |
                    [Unit]
                    Description=Toolbox VPN Init
                    After=docker.service
                    Requires=docker.service
                    Before=openvpn.service
                    ConditionPathExists=!/home/core/vpn/openvpn.conf

                    [Service]
                    Type=oneshot
                    TimeoutStartSec=0
                    Environment=/etc/environment
                    ExecStart=/usr/bin/docker run -v /home/core/vpn:/etc/openvpn --rm kylemanna/openvpn ovpn_genconfig -u udp://$public_ipv4:1194 \
                                                                                                                       -n 10.0.2.2 \
                                                                                                                       -n 185.23.94.244 \
                                                                                                                       -n 8.8.8.8 \
                                                                                                                       -s 10.8.0.0/24 \
                                                                                                                       -N \
                                                                                                                       -p "route 10.0.2.0 255.255.255.0" \
                                                                                                                       -p "route 10.0.3.0 255.255.255.0" \
                                                                                                                       -p "dhcp-option DOMAIN-SEARCH $domain" \
                                                                                                                       -p "dhcp-option DOMAIN-SEARCH svc.$domain" \
                                                                                                                       -p "dhcp-option DOMAIN-SEARCH default.svc.$domain"
                    ExecStart=/usr/bin/docker run -v /home/core/vpn:/etc/openvpn -e "EASYRSA_BATCH=1" -e "EASYRSA_REQ_CN=$domain" --rm kylemanna/openvpn ovpn_initpki nopass
                    ExecStart=/usr/bin/docker run -v /home/core/vpn:/etc/openvpn --rm kylemanna/openvpn easyrsa build-client-full cloud nopass
                    ExecStart=/bin/bash -c "/usr/bin/docker run -v /home/core/vpn:/etc/openvpn --rm kylemanna/openvpn ovpn_getclient cloud > /home/core/cloud.ovpn"
                - name: openvpn.service
                  command: start
                  content: |
                    [Unit]
                    Description=Toolbox VPN
                    After=docker.service openvpn-init.service
                    Requires=docker.service openvpn-init.service

                    [Service]
                    TimeoutStartSec=0
                    Environment=/etc/environment
                    ExecStartPre=-/usr/bin/docker kill openvpn
                    ExecStartPre=-/usr/bin/docker rm openvpn
                    ExecStartPre=/usr/bin/docker pull kylemanna/openvpn
                    ExecStart=/usr/bin/docker run -v /home/core/vpn:/etc/openvpn --name=openvpn -p 1194:1194/udp --cap-add=NET_ADMIN --net=host kylemanna/openvpn
                    ExecStop=/usr/bin/docker stop openvpn
  floating_ip:
    type: OS::Neutron::FloatingIP
    properties:
      floating_network_id: 6ea98324-0f14-49f6-97c0-885d1b8dc517

  floating_ip_link:
    type: OS::Nova::FloatingIPAssociation
    properties:
      floating_ip: { get_resource: floating_ip }
      server_id: { get_resource: master }

  nodes:
    type: OS::Heat::ResourceGroup
    properties:
      count: 1
      resource_def:
        type: kube-node.heat.yaml
        properties:
          domain: { get_param: domain }
          master: { get_attr: [ master_port, fixed_ips, 0, ip_address ] }
          security_group: { get_resource: security_group }
          network: { get_resource: network }
          nodename: node-%index%
          keypair_name: { get_param: keypair_name }
          flavor_name: { get_param: flavor_name }
