#
##
### Written by the CAT team
##
#
heat_template_version: 2013-05-23


description: High Aviability Toolbox stack for Cloudwatt


parameters:
  keypair_name:
    description: Keypair to inject in instance
    label: SSH Keypair
    type: string

  os_username:
    description: OpenStack Username
    label: OpenStack Username
    type: string

  os_password:
    description: OpenStack Password
    label: OpenStack Password
    type: string

  os_tenant:
    description: OpenStack Tenant Name
    label: OpenStack Tenant Name
    type: string

  os_auth:
    description: OpenStack Auth URL
    default: https://identity.fr1.cloudwatt.com/v2.0
    label: OpenStack Auth URL
    type: string

  domain:
    description: Wildcarded domain, ex example.com must have a *.example.com DNS entry
    label: Cloud DNS
    type: string

  flavor_name:
    default: n2.cw.standard-4
    description: Flavor to use for the deployed instance
    type: string
    label: Instance Type (Flavor)
    constraints:
      - allowed_values:
          - s1.cw.small-1
          - n1.cw.standard-1
          - n1.cw.standard-2
          - n2.cw.standard-4
          - n1.cw.standard-8
          - n1.cw.standard-12
          - n1.cw.standard-16

resources:
  network:
    type: OS::Neutron::Net

  subnet:
    type: OS::Neutron::Subnet
    properties:
      network_id: { get_resource: network }
      ip_version: 4
      dns_nameservers:
        - 185.23.94.244
        - 185.23.94.245
      cidr: 10.0.1.0/24
      allocation_pools:
        - { start: 10.0.1.100, end: 10.0.1.199 }

  router:
    type: OS::Neutron::Router
    properties:
      admin_state_up: true
      external_gateway_info:
        enable_snat: true
        network: "public"

  toolbox_interface:
    type: OS::Neutron::RouterInterface
    properties:
      router_id: { get_resource: router }
      subnet_id: { get_resource: subnet }

  security_group:
    type: OS::Neutron::SecurityGroup
    properties:
      rules:
        - { direction: ingress }
        - { direction: egress }

  floating_ip:
    type: OS::Neutron::FloatingIP
    properties:
      floating_network_id: 6ea98324-0f14-49f6-97c0-885d1b8dc517

  ports:
    type: OS::Neutron::Port
    properties:
      network: { get_resource: network }
      security_groups:
        - { get_resource: security_group }

  server:
    type: OS::Nova::Server
    properties:
      key_name: { get_param: keypair_name }
      image: 811c20f7-ad34-4401-85a9-72b9f921a0dd
      flavor: { get_param: flavor_name }
      user_data_format: RAW
      networks:
        - port: { get_resource: ports }
      user_data:
        str_replace:
          params:
            $private_ipv4: { get_attr: [ ports, fixed_ips, 0, ip_address ] }
            $public_ipv4: { get_attr: [floating_ip, floating_ip_address] }
            $domain: { get_param: domain }
            $os_username: { get_param: os_username }
            $os_password: { get_param: os_password }
            $os_tenant: { get_param: os_tenant }
            $os_auth: { get_param: os_auth }
          template: |
            #cloud-config
            write_files:
              - path: /etc/flannel/options.env
                permissions: 0644
                owner: "root:root"
                content: |
                  FLANNELD_IFACE=$private_ipv4
                  FLANNELD_ETCD_ENDPOINTS=http://$private_ipv4:2379
              - path: /opt/flannel-init.sh
                permissions: 0700
                owner: "root:root"
                content: |
                  #!/bin/bash
                  echo "Waiting for etcd..."
                  ETCD="http://$private_ipv4:2379"
                  while true
                  do
                      echo "Trying: $ETCD"
                      if [ -n "$(curl --silent "$ETCD/v2/machines")" ]; then
                          ACTIVE_ETCD=$ETCD
                          break
                      fi
                      sleep 1
                      if [ -n "$ACTIVE_ETCD" ]; then
                          break
                      fi
                  done
                  RES=$(curl --silent -X PUT -d "value={\"Network\":\"10.0.3.0/24\",\"Backend\":{\"Type\":\"vxlan\"}}" "$ACTIVE_ETCD/v2/keys/coreos.com/network/config?prevExist=false")
                  if [ -z "$(echo $RES | grep '"action":"create"')" ] && [ -z "$(echo $RES | grep 'Key already exists')" ]; then
                      echo "Unexpected error configuring flannel pod network: $RES"
                  fi
              - path: /opt/kube-namespace-init.sh
                permissions: 0700
                owner: "root:root"
                content: |
                  #!/bin/bash
                  echo "Waiting for Kubernetes API..."
                  K8S="http://$private_ipv4:8080"
                  while true
                  do
                      echo "Trying: $K8S"
                      if [ -n "$(curl --silent "$K8S/version")" ]; then
                          ACTIVE_K8S=$K8S
                          break
                      fi
                      sleep 1
                      if [ -n "$ACTIVE_K8S" ]; then
                          break
                      fi
                  done
                  RES=$(curl -H "Content-Type: application/json" -XPOST -d'{"apiVersion":"v1","kind":"Namespace","metadata":{"name":"kube-system"}}' "http://127.0.0.1:8080/api/v1/namespaces")
                  if [ -z "$(echo $RES | grep '"phase": "Active"')" ]; then
                      echo "Unexpected error configuring Kubernetes Namespace : $RES"
                  else
                      echo "Created kube-system namespace"
                      mkdir -p /opt/bin
                      curl -o /opt/bin/kubectl https://storage.googleapis.com/kubernetes-release/release/v1.2.0/bin/linux/amd64/kubectl
                      chmod +x /opt/bin/kubectl
                      kubectl create -f /etc/kubernetes/descriptors
                  fi
              - path: /etc/kubernetes/manifests/kube-apiserver.yaml
                permissions: 0666
                owner: "root:root"
                content: |
                  apiVersion: v1
                  kind: Pod
                  metadata:
                    name: kube-apiserver
                    namespace: kube-system
                  spec:
                    hostNetwork: true
                    containers:
                    - name: kube-apiserver
                      image: quay.io/coreos/hyperkube:v1.2.0_coreos.0
                      command:
                      - /hyperkube
                      - apiserver
                      - --bind-address=0.0.0.0
                      - --insecure-bind-address=0.0.0.0
                      - --etcd-servers=http://$private_ipv4:2379
                      - --allow-privileged=true
                      - --service-cluster-ip-range=10.0.2.0/24
                      - --secure-port=443
                      - --advertise-address=$private_ipv4
                      - --admission-control=NamespaceLifecycle,NamespaceExists,LimitRanger,SecurityContextDeny,ServiceAccount,ResourceQuota
                      - --tls-cert-file=/etc/kubernetes/ssl/apiserver.pem
                      - --tls-private-key-file=/etc/kubernetes/ssl/apiserver-key.pem
                      - --client-ca-file=/etc/kubernetes/ssl/ca.pem
                      - --service-account-key-file=/etc/kubernetes/ssl/apiserver-key.pem
                      ports:
                      - containerPort: 443
                        hostPort: 443
                        name: https
                      - containerPort: 8080
                        hostPort: 8080
                        name: local
                      volumeMounts:
                      - mountPath: /etc/kubernetes/ssl
                        name: ssl-certs-kubernetes
                        readOnly: true
                      - mountPath: /etc/ssl/certs
                        name: ssl-certs-host
                        readOnly: true
                    volumes:
                    - hostPath:
                        path: /etc/kubernetes/ssl
                      name: ssl-certs-kubernetes
                    - hostPath:
                        path: /usr/share/ca-certificates
                      name: ssl-certs-host
              - path: /etc/kubernetes/manifests/kube-proxy.yaml
                permissions: 0666
                owner: "root:root"
                content: |
                  apiVersion: v1
                  kind: Pod
                  metadata:
                    name: kube-proxy
                    namespace: kube-system
                  spec:
                    hostNetwork: true
                    containers:
                    - name: kube-proxy
                      image: quay.io/coreos/hyperkube:v1.2.0_coreos.0
                      command:
                      - /hyperkube
                      - proxy
                      - --master=http://127.0.0.1:8080
                      - --proxy-mode=iptables
                      securityContext:
                        privileged: true
                      volumeMounts:
                      - mountPath: /etc/ssl/certs
                        name: ssl-certs-host
                        readOnly: true
                    volumes:
                    - hostPath:
                        path: /usr/share/ca-certificates
                      name: ssl-certs-host
              - path: /etc/kubernetes/manifests/kube-podmaster.yaml
                permissions: 0666
                owner: "root:root"
                content: |
                  apiVersion: v1
                  kind: Pod
                  metadata:
                    name: kube-podmaster
                    namespace: kube-system
                  spec:
                    hostNetwork: true
                    containers:
                    - name: scheduler-elector
                      image: gcr.io/google_containers/podmaster:1.1
                      command:
                      - /podmaster
                      - --etcd-servers=http://$private_ipv4:2379
                      - --key=scheduler
                      - --whoami=${ADVERTISE_IP}
                      - --source-file=/src/manifests/kube-scheduler.yaml
                      - --dest-file=/dst/manifests/kube-scheduler.yaml
                      volumeMounts:
                      - mountPath: /src/manifests
                        name: manifest-src
                        readOnly: true
                      - mountPath: /dst/manifests
                        name: manifest-dst
                    - name: controller-manager-elector
                      image: gcr.io/google_containers/podmaster:1.1
                      command:
                      - /podmaster
                      - --etcd-servers=http://$private_ipv4:2379
                      - --key=controller
                      - --whoami=$private_ipv4
                      - --source-file=/src/manifests/kube-controller-manager.yaml
                      - --dest-file=/dst/manifests/kube-controller-manager.yaml
                      terminationMessagePath: /dev/termination-log
                      volumeMounts:
                      - mountPath: /src/manifests
                        name: manifest-src
                        readOnly: true
                      - mountPath: /dst/manifests
                        name: manifest-dst
                    volumes:
                    - hostPath:
                        path: /srv/kubernetes/manifests
                      name: manifest-src
                    - hostPath:
                        path: /etc/kubernetes/manifests
                      name: manifest-dst
              - path: /etc/kubernetes/manifests/kube-controller-manager.yaml
                permissions: 0666
                owner: "root:root"
                content: |
                  apiVersion: v1
                  kind: Pod
                  metadata:
                    name: kube-controller-manager
                    namespace: kube-system
                  spec:
                    hostNetwork: true
                    containers:
                    - name: kube-controller-manager
                      image: quay.io/coreos/hyperkube:v1.2.0_coreos.0
                      command:
                      - /hyperkube
                      - controller-manager
                      - --master=http://127.0.0.1:8080
                      - --service-account-private-key-file=/etc/kubernetes/ssl/apiserver-key.pem
                      - --root-ca-file=/etc/kubernetes/ssl/ca.pem
                      livenessProbe:
                        httpGet:
                          host: 127.0.0.1
                          path: /healthz
                          port: 10252
                        initialDelaySeconds: 15
                        timeoutSeconds: 1
                      volumeMounts:
                      - mountPath: /etc/kubernetes/ssl
                        name: ssl-certs-kubernetes
                        readOnly: true
                      - mountPath: /etc/ssl/certs
                        name: ssl-certs-host
                        readOnly: true
                    volumes:
                    - hostPath:
                        path: /etc/kubernetes/ssl
                      name: ssl-certs-kubernetes
                    - hostPath:
                        path: /usr/share/ca-certificates
                      name: ssl-certs-host
              - path: /etc/kubernetes/manifests/kube-scheduler.yaml
                permissions: 0666
                owner: "root:root"
                content: |
                  apiVersion: v1
                  kind: Pod
                  metadata:
                    name: kube-scheduler
                    namespace: kube-system
                  spec:
                    hostNetwork: true
                    containers:
                    - name: kube-scheduler
                      image: quay.io/coreos/hyperkube:v1.2.0_coreos.0
                      command:
                      - /hyperkube
                      - scheduler
                      - --master=http://127.0.0.1:8080
                      livenessProbe:
                        httpGet:
                          host: 127.0.0.1
                          path: /healthz
                          port: 10251
                        initialDelaySeconds: 15
                        timeoutSeconds: 1
              - path: /etc/kubernetes/descriptors/rabbitmq-service.yaml
                permissions: 0666
                owner: "root:root"
                content: |
                  apiVersion: v1
                  kind: Service
                  metadata:
                    labels:
                      component: rabbitmq
                    name: rabbitmq-service
                  spec:
                    ports:
                      - port: 5672
                    selector:
                      component: rabbitmq
              - path: /etc/kubernetes/descriptors/rabbitmq-rc.yaml
                permissions: 0666
                owner: "root:root"
                content: |
                  apiVersion: v1
                  kind: ReplicationController
                  metadata:
                    labels:
                      component: rabbitmq
                    name: rabbitmq-controller
                  spec:
                    replicas: 1
                    template:
                      metadata:
                        labels:
                          component: rabbitmq
                      spec:
                        containers:
                          - image: rabbitmq
                            name: rabbitmq
                            ports:
                              - containerPort: 5672
                                hostPort: 5672
                            resources:
                              limits:
                                cpu: 100m
              - path: /etc/kubernetes/descriptors/toolbox-service.yaml
                permissions: 0666
                owner: "root:root"
                content: |
                  apiVersion: v1
                  kind: Service
                  metadata:
                    labels:
                      component: toolbox
                    name: toolbox-service
                  spec:
                    ports:
                      - port: 3000
                        nodePort: 80
                    selector:
                      component: toolbox
              - path: /etc/kubernetes/descriptors/toolbox-rc.yaml
                permissions: 0666
                owner: "root:root"
                content: |
                  apiVersion: v1
                  kind: ReplicationController
                  metadata:
                    labels:
                      component: toolbox
                    name: toolbox-controller
                  spec:
                    replicas: 1
                    template:
                      metadata:
                        labels:
                          component: toolbox
                      spec:
                        containers:
                          - image: cloudwattfr/toolbox:latest
                            name: toolbox
                            ports:
                              - containerPort: 3000
              - path: /etc/kubernetes/descriptors/rethinkdb-service.yaml
                permissions: 0666
                owner: "root:root"
                content: |
                  apiVersion: v1
                  kind: Service
                  metadata:
                    labels:
                      db: rethinkdb
                    name: rethinkdb-driver
                  spec:
                    ports:
                      - port: 28015
                        targetPort: 28015
                    selector:
                      db: rethinkdb
              - path: /etc/kubernetes/descriptors/rethinkdb-rc.yaml
                permissions: 0666
                owner: "root:root"
                content: |
                  apiVersion: v1
                  kind: ReplicationController
                  metadata:
                    labels:
                      db: rethinkdb
                    name: rethinkdb-rc
                  spec:
                    replicas: 1
                    selector:
                      db: rethinkdb
                      role: replicas
                    template:
                      metadata:
                        labels:
                          db: rethinkdb
                          role: replicas
                      spec:
                        containers:
                          - image: cedbossneo/rethinkdb-kubernetes:latest
                            name: rethinkdb
                            env:
                              - name: POD_NAMESPACE
                                valueFrom:
                                  fieldRef:
                                    fieldPath: metadata.namespace
                            ports:
                              - containerPort: 8080
                                name: admin-port
                                hostPort: 8081
                              - containerPort: 28015
                                name: driver-port
                                hostPort: 28015
                              - containerPort: 29015
                                name: cluster-port
                            volumeMounts:
                              - mountPath: /data/rethinkdb_data
                                name: rethinkdb-storage
                        volumes:
                          - name: rethinkdb-storage
                            emptyDir: {}
              - path: /etc/environment
                permissions: 0644
                owner: "root:root"
                content: |
                  COREOS_PRIVATE_IPV4=$private_ipv4
                  COREOS_PUBLIC_IPV4=$public_ipv4
                  ETCD_ADDR=$private_ipv4:2379
                  ETCD_PEER_ADDR=$private_ipv4:2380
                  TOOLBOX_DOMAIN=$domain
              - path: /opt/kubernetes-init-ssl.sh
                permissions: 0700
                owner: "root:root"
                content: |
                  #!/bin/bash
                  mkdir -p /etc/kubernetes/ssl
                  cd /etc/kubernetes/ssl
                  # Config
                  cat <<EOF > openssl.cnf
                  [req]
                  req_extensions = v3_req
                  distinguished_name = req_distinguished_name
                  [req_distinguished_name]
                  [ v3_req ]
                  basicConstraints = CA:FALSE
                  keyUsage = nonRepudiation, digitalSignature, keyEncipherment
                  subjectAltName = @alt_names
                  [alt_names]
                  DNS.1 = kubernetes
                  DNS.2 = kubernetes.default
                  DNS.3 = kubernetes.default.svc
                  DNS.4 = kubernetes.default.svc.cluster.local
                  IP.1 = 10.0.2.1
                  IP.2 = $private_ipv4
                  EOF
                  cat <<EOF > worker-openssl.cnf
                  [req]
                  req_extensions = v3_req
                  distinguished_name = req_distinguished_name
                  [req_distinguished_name]
                  [ v3_req ]
                  basicConstraints = CA:FALSE
                  keyUsage = nonRepudiation, digitalSignature, keyEncipherment
                  subjectAltName = @alt_names
                  [alt_names]
                  IP.1 = \$ENV::WORKER_IP
                  EOF
                  # Root CA
                  openssl genrsa -out ca-key.pem 2048
                  openssl req -x509 -new -nodes -key ca-key.pem -days 10000 -out ca.pem -subj "/CN=kube-ca"
                  # Server CA
                  openssl genrsa -out apiserver-key.pem 2048
                  openssl req -new -key apiserver-key.pem -out apiserver.csr -subj "/CN=kube-apiserver" -config openssl.cnf
                  openssl x509 -req -in apiserver.csr -CA ca.pem -CAkey ca-key.pem -CAcreateserial -out apiserver.pem -days 365 -extensions v3_req -extfile openssl.cnf
                  # Worker CA
                  export WORKER_FQDN=toolbox
                  export WORKER_IP=$private_ipv4
                  openssl genrsa -out ${WORKER_FQDN}-worker-key.pem 2048
                  openssl req -new -key ${WORKER_FQDN}-worker-key.pem -out ${WORKER_FQDN}-worker.csr -subj "/CN=${WORKER_FQDN}" -config worker-openssl.cnf
                  openssl x509 -req -in ${WORKER_FQDN}-worker.csr -CA ca.pem -CAkey ca-key.pem -CAcreateserial -out ${WORKER_FQDN}-worker.pem -days 365 -extensions v3_req -extfile worker-openssl.cnf
                  # Admin CA
                  openssl genrsa -out admin-key.pem 2048
                  openssl req -new -key admin-key.pem -out admin.csr -subj "/CN=kube-admin"
                  openssl x509 -req -in admin.csr -CA ca.pem -CAkey ca-key.pem -CAcreateserial -out admin.pem -days 365
                  # Set permissions
                  chmod 600 /etc/kubernetes/ssl/*-key.pem
                  chown root:root /etc/kubernetes/ssl/*-key.pem

            coreos:
              etcd2:
                name: "%H"
                advertise-client-urls: http://$private_ipv4:2379
                initial-advertise-peer-urls: http://$private_ipv4:2380
                initial-cluster: "%H=http://$private_ipv4:2380"
                listen-client-urls: http://0.0.0.0:2379,http://0.0.0.0:4001
                listen-peer-urls: http://$private_ipv4:2380
              units:
                - name: etcd2.service
                  command: start
                - name: generatekey.service
                  command: start
                  content: |
                    [Unit]
                    ConditionPathExists=!/home/core/keys/key
                    Description=Toolbox KeyPair Generator

                    [Service]
                    Type=oneshot
                    ExecStartPre=-/usr/bin/mkdir /home/core/keys
                    ExecStart=/usr/bin/ssh-keygen -t rsa -f /home/core/keys/key -N ""
                - name: generatessl.service
                  command: start
                  content: |
                    [Unit]
                    ConditionPathExists=!/etc/kubernetes/ssl
                    Description=Kubernetes Keys Generator

                    [Service]
                    Type=oneshot
                    ExecStart=/opt/kubernetes-init-ssl.sh
                - name: flanneld.service
                  drop-ins:
                    - name: 40-ExecStartPre-symlink.conf
                      content: |
                        [Service]
                        ExecStartPre=/opt/flannel-init.sh
                        ExecStartPre=/usr/bin/ln -sf /etc/flannel/options.env /run/flannel/options.env
                - name: docker.service
                  drop-ins:
                    - name: 40-flannel.conf
                      content: |
                        [Unit]
                        Requires=flanneld.service
                        After=flanneld.service
                - name: kubelet.service
                  command: start
                  content: |
                    [Service]
                    ExecStartPre=/usr/bin/mkdir -p /etc/kubernetes/manifests
                    Environment=KUBELET_VERSION=v1.2.0_coreos.0
                    ExecStart=/usr/lib/coreos/kubelet-wrapper \
                      --api-servers=http://127.0.0.1:8080 \
                      --register-node=true \
                      --allow-privileged=true \
                      --config=/etc/kubernetes/manifests \
                      --hostname-override=$private_ipv4 \
                      --cluster-dns=10.2.0.2 \
                      --cluster-domain=cluster.local
                    ExecStartPost=-/opt/kube-namespace-init.sh
                    Restart=always
                    RestartSec=10
                    [Install]
                    WantedBy=multi-user.target
  floating_ip_link:
    type: OS::Nova::FloatingIPAssociation
    properties:
      floating_ip: { get_resource: floating_ip }
      server_id: { get_resource: server }

outputs:
  floating_ip:
    description: IP address of the deployed compute instance
    value: { get_attr: [floating_ip, floating_ip_address] }
